Pearless Patents : Exploratory Data Analytics Project
1. Introduction :
A leading IP law firm specializing with patent, trademark and design services
wanted to perform a large scale data mining of global patents. One of the tasks
was to identify the exceptional patents that have been cited the most. Using a
conventional solution meant prohibitively slow execution of algorithms due to a)
existing high volumes of documents, b) frequent updates and c) ongoing new
additions to the repository. Experts on board analyzed this big data problem, and
decided to leverage exploratory data analysis techniques to analyze the citation
using MapReduce and extract a list of peerless patents that were cited extensively.
The IP services would eventually use this list as a ready reckoner for their
consulting services.

2. Inference :
The consulting arm of law firm shall use the output generated by the project for as a
ready reference for their ace projects for global markets.

3. Assumptions :
(a) It is assumed that the output of this project is going to be used by the
Consulting arm of IP law firm.
(b)  The output generated from this project would be in CSV format.

4. Requirements :
It is required to identify a set of ‘Exceptional Patents’ on an ongoing basis using an
Exploratory Data Analysis (EDA) technique to spot outliers. In statistics, an outlier is
a point that is distant from the other observations. In the current challenge, outliers
can be treated as global patents whose count of citations is exceptionally high
compared to rest of the patents. Such a list of exceptionally cited patents is required
to facilitate quick reference for patent cases being filed by the IP consultants for
their global clientele. The essential concepts of EDA technique are explained below:

(a) Outlier Detection :
The goal of Exploratory Data Analysis (EDA) is to keep analyzing the data till
patterns emerge. The term ‘Exploratory’ intuitively suggests that as a data analyst
you are about to wear a detective hat to understand what’s happening where in the
data and iteratively work towards finding patterns that define the characteristics of
the data being analyzed. The analysis begins with understanding the spread, the
middle point, middle swell and the tails. Any values lying outside are likely to catch
attention. The challenge in this project requires finding those patents that have been
cited exceptionally high number of times.
In order to determine the outlier values, it involves computing the 5 number
summary and inter-quartile range (IQR) after sorting the ’count of ratings’ given by
customers. This is usually represented in graphically in form of a box plot. The box
in the box plot represents the inter quartile range (IQR = Q3-Q1) where its left
border (also called lower or left hinge) corresponds to the first quartile (Q1) and the
right border (also called upper or right hinge) corresponds to the third quartile (Q3).
Therefore, the middle 50% of data values fall within the box. The line in the middle
represents the median of the data. The left whisker represents the smallest 25% of
data values with its left most end corresponding to the minimum value of the data.
Similarly, the right whisker represents the largest 25% of data values with its right
most end corresponding to the maximum value of the data.

If both the whiskers are much longer than the length of the box (IQR), it is an
indication of the possible presence of outliers. In fact, Tukey suggests that an outlier
is a point that is greater than or less than 1.5 times the IQR.
For this project, consider the outlier values as >3rd Quartile + (1.5 x IQR)
Where, IQR = 3rd Quartile – 1st Quartile.
It is recommended to use Tukey’s method of box plot analysis to explore and
understand data characteristics along with identification of outliers. Read more
about EDA and outlier detection method at
http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm . 
The broad steps involved in discovering the “exceptionally cited patents” are outlined below:
Using MapReduce count the number of citing for every patent.
1. Sort patents ratings data by leveraging MapReduce.
2. Using the sorted list, generate the 5 number summaries of Min, Max, Median 1st
Quartile and the 3rd Quartile.
3. The outliers can be taken as values falling outside the lower outer fence values.
Refer to the above formula.
To summarize, the outlier detection will provide a list of global patents having
number of citations outside the normal citation pattern. These patents can be
classified, as ‘Peerless Patents’ and their citations are an indication of their being
most sought after research content across the globe.

5. Data Sourcing Guidelines :
Big data solutions solve problems by ingesting extremely large of volumes of data
for various operations to be carried out on them before the results are shared with
the end user or the stream of output is generated for another application’s input.
The following guidelines would come in handy to source the data for your projects.
Download the data from the following URL and save it on into the local drive to be
reviewed before moving into HDFS.
Overview : http://bit.ly/1DSp43j
Data: http://bit.ly/1ISMAJY

6. Testing Guidelines :
It’s easy to think that, if we know how to test a standard application, we know how to
test the Big Data storage and application. Surprisingly so, it’s not the case! Volume,
Variety and Velocity of data make things really complex to test. While testing,
mostly you are not dealing with structured data with a fixed schema; mostly the data
is unstructured and a loosely defined or dynamic schema. The rate at which data is
generated clearly exerts a pressure on speed of processing. Following must be kept
in mind while planning the testing:
1. Plan on unit testing early and frequently during development. This is simply
because big data testing is challenging, you may not be able to view source
data using spreadsheets owing to sheer magnitude of the data.
2. Do not rely on eyeballing data or outputs as mechanism for verification.
Create Test plan for each data set and the transformations stages it will go
through in the entire process.
3. Big Data developers and testing team have to work with’ Unstructured or
Semi Structured’ data (Data with dynamic schema) most of the time. Thus
the testing activity requires additional inputs on ‘how to derive the structure
dynamically from the given data sources’ from the business/development
teams.
4. When it comes to the actual validation of the data, considering the huge data
sets for validation, ‘Sampling’ strategy comes to rescue. But even that is a
challenge in the context of Big Data Validation. This provides a tremendous
opportunity for the testers who are innovative and who would go the extra
mile to build the utilities that can increase the test coverage of BIG Data
while increasing the test productivity as well.
5. The testing process should be strengthened on reuse and optimization of
the test case sets, otherwise due to sheer size of the requirements to be
tested will become unmanageable.
